{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Adam Lewandowski, Ivan Sladkov, Patrick English\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3  \\\n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets_df = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", header=None)\n",
    "raw_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline processing\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "#This can take a while\n",
    "#tokenized_tweets = raw_tweets_df[5].copy().apply(lambda x : x.lower())\n",
    "\n",
    "# If you don't want to wait, vary slice value\n",
    "slice_ = int(len(raw_tweets_df)*0.1)\n",
    "tokenized_tweets = raw_tweets_df[5][:slice_].copy().apply(lambda x : x.lower().encode('ascii', 'ignore'))\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x : tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(tweets):\n",
    "    t = tweets.copy()\n",
    "    def remove_punct(tokens):\n",
    "        if '.' in tokens:\n",
    "            tokens.remove('.')\n",
    "        if '?' in tokens:\n",
    "            tokens.remove('?')\n",
    "        if '!' in tokens:\n",
    "            tokens.remove('!')\n",
    "        if ',' in tokens:\n",
    "            tokens.remove(',')\n",
    "        if '\\'' in tokens:\n",
    "            tokens.remove('\\'')\n",
    "        if '\"' in tokens:\n",
    "            tokens.remove('\"')\n",
    "        if '#' in tokens:\n",
    "            tokens.remove('#')\n",
    "        if '' in tokens:\n",
    "            tokens.remove('#')\n",
    "            \n",
    "        return tokens\n",
    "    return t.apply(lambda tweet : remove_punct(tweet))\n",
    "\n",
    "def remove_links(tweets):\n",
    "    return tweets\n",
    "\n",
    "def remove_stop_words(tweets):\n",
    "    t = tweets.copy()\n",
    "    from nltk.corpus import stopwords\n",
    "    sw = set(stopwords.words('english')) \n",
    "    return t.apply(lambda tweet:[w for w in tweet if not w in sw])\n",
    "\n",
    "def merge_tweets(tweets):\n",
    "    t = tweets.copy()\n",
    "    return t.apply(lambda tweet: ' '.join(tweet))\n",
    "\n",
    "def pos_tag(tweets):\n",
    "    t = tweets.copy()\n",
    "    return t.apply(lambda tweet: nltk.pos_tag(tweet))\n",
    "\n",
    "def merge_lists(tweets):\n",
    "    list_ = tweets.tolist()\n",
    "    return [j for i in list_ for j in i]\n",
    "\n",
    "#TBD\n",
    "def emoticon_transcoder(tweets):\n",
    "    return tweets\n",
    "\n",
    "def remove_pos_tags(tweets):\n",
    "    t = tweets.copy()\n",
    "    return t.apply(lambda tweet: [w[0] for w in tweet])\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "def scikit_vectorize(tweets, min_freq):\n",
    "    t = tweets.copy()\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, tokenizer=identity_tokenizer, min_df=min_freq, dtype=np.float32)\n",
    "    vectors = vectorizer.fit_transform(t)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    return pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "def scikit_hash_vectorize(tweets, features):\n",
    "    t = tweets.copy()\n",
    "    vectorizer = HashingVectorizer(lowercase=False, n_features=features, tokenizer=identity_tokenizer, dtype=np.float32)\n",
    "    vectors = vectorizer.fit_transform(t)\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    return pd.DataFrame(denselist)\n",
    "\n",
    "def str_list_to_list(tweets):\n",
    "    t = tweets.copy()\n",
    "    import ast \n",
    "    t[\"tweets\"] = t[\"tweets\"].apply(lambda x: ast.literal_eval(x))\n",
    "    return t\n",
    "\n",
    "def transcode_emoticons(tweets):\n",
    "    t = tweets.copy()\n",
    "    eyes = [\";\",\":\",\"=\", \"8-\"]\n",
    "    positive_jaw = [\")\", \"]\",\"}\",\">\",\"3\",\"d\",\"*\"] \n",
    "    negative_jaw = [\"(\",\"[\",\"{\",\"<\",\"/\",\"\\\\\",\"c\"]\n",
    "    surprised_jaw = [\"o\"]\n",
    "    tounge_jaw = [\"p\"]\n",
    "    def transcode_emoticon(tweet):\n",
    "        for i, word in enumerate(tweet):\n",
    "            eye_present = any(eye in eyes for eye in word)\n",
    "            if word == \"x-d\" or word == \"xd\":\n",
    "                tweet[i] = \":)\"\n",
    "            elif eye_present and any(jaw in positive_jaw for jaw in word):\n",
    "                tweet[i] = \":)\"\n",
    "            elif eye_present and any(jaw in negative_jaw for jaw in word):\n",
    "                tweet[i] = \":(\"\n",
    "            elif eye_present and any(jaw in surprised_jaw for jaw in word):\n",
    "                tweet[i] = \":o\"\n",
    "            elif eye_present and any(jaw in tounge_jaw for jaw in word):\n",
    "                tweet[i] = \":p\"\n",
    "        return tweet\n",
    "    return t.apply(lambda tweet: transcode_emoticon(tweet))\n",
    "\n",
    "def miniscule_transcoder(tweets):\n",
    "    t = tweets.copy()\n",
    "    def transcode_tweet(tweet):\n",
    "        for i, word in enumerate(tweet):\n",
    "            if \"haha\" in word or \"hehe\" in word:\n",
    "                tweet[i] = \"ha\"\n",
    "            elif word.startswith(\"http\"):\n",
    "                tweet[i] = \"httpLink\"\n",
    "            elif word == \"im\":\n",
    "                tweet[i] = \"i'm\"\n",
    "            elif word ==\"u\":\n",
    "                tweet[i] = \"you\"\n",
    "            elif word == \"cant\":\n",
    "                tweet[i] = \"can't\"\n",
    "            elif word == \"thats\":\n",
    "                tweet[i] = \"that's\"\n",
    "            elif \"aww\" in word:\n",
    "                tweet[i] = \"aww\"\n",
    "            elif word.startswith(\"@\"):\n",
    "                tweet[i] =\"@\"\n",
    "            elif word ==\"ppl\":\n",
    "                tweet[i] = \"people\"\n",
    "            elif \"..\" in word:\n",
    "                tweet[i] = \"...\"\n",
    "                \n",
    "        return tweet\n",
    "    return t.apply(lambda tweet : transcode_tweet(tweet))\n",
    "\n",
    "def lemmatize(tweets):\n",
    "    t = tweets.copy()\n",
    "    \n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        from nltk.corpus import wordnet\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    lem = WordNetLemmatizer()\n",
    "    def lemmatize_word(w):\n",
    "        transcoded_tag = get_wordnet_pos(w[1])\n",
    "        wl = lem.lemmatize(w[0], transcoded_tag)\n",
    "        return (wl,w[1])\n",
    "    return t.apply(lambda tweet:[lemmatize_word(w) for w in tweet])\n",
    "\n",
    "def remove_punct(tokens):\n",
    "        if '.' in tokens:\n",
    "            tokens.remove('.')\n",
    "        if '?' in tokens:\n",
    "            tokens.remove('?')\n",
    "        if '!' in tokens:\n",
    "            tokens.remove('!')\n",
    "        if ',' in tokens:\n",
    "            tokens.remove(',')\n",
    "        return tokens\n",
    "    \n",
    "def remove_one_char_trash(tweets):\n",
    "    return tweets.apply(lambda tweet: [s for s in tweet if len(s) >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "#comment lines to skip some steps\n",
    "#You need to run things in this order\n",
    "tweets = remove_punctuation(tokenized_tweets)\n",
    "tweets = remove_one_char_trash(tweets)\n",
    "tweets = remove_stop_words(tweets)\n",
    "tweets = pos_tag(tweets)\n",
    "tweets = lemmatize(tweets)\n",
    "tweets = remove_pos_tags(tweets)\n",
    "tweets = miniscule_transcoder(tweets)\n",
    "tweets = transcode_emoticons(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@', 59727), ('...', 42591), (\"i'm\", 20124), ('go', 19523), ('get', 18863), ('work', 12768), ('day', 10853), (\"can't\", 8833), ('miss', 8756), ('like', 8403), ('today', 8062), ('want', 7684), ('back', 7358), ('good', 6856), ('feel', 6628), ('time', 6554), ('think', 6102), ('really', 6074), ('still', 6037), ('one', 5529), ('know', 5416), ('need', 5358), ('well', 5269), ('make', 5255), ('wish', 5249), ('sleep', 5240), ('sad', 5190), ('home', 5002), ('bad', 4966), ('last', 4881), ('see', 4880), ('night', 4728), ('httpLink', 4648), ('oh', 4440), ('come', 4407), ('lol', 4354), ('sorry', 4078), ('tomorrow', 4062), ('much', 3987), ('love', 3776), ('morning', 3704), ('hate', 3685), ('look', 3602), ('watch', 3588), ('week', 3538), ('school', 3537), ('ha', 3354), ('leave', 3326), ('take', 3260), ('sick', 3085), ('hope', 3079), ('say', 3030), ('try', 3029), ('though', 3022), ('bed', 2952), ('find', 2869), ('twitter', 2869), ('hour', 2841), ('right', 2829), ('new', 2825), ('thing', 2690), ('could', 2656), (\"that's\", 2640), ('would', 2637), ('even', 2616), ('start', 2569), ('lose', 2568), ('suck', 2555), ('wait', 2528), ('dont', 2488), ('weekend', 2475), ('way', 2467), ('rain', 2448), ('gonna', 2397), ('hurt', 2387), ('tonight', 2358), (\"i've\", 2320), ('people', 2262), ('never', 2206), ('long', 2195), ('tire', 2167), (':)', 2135), ('aww', 2125), ('wanna', 2120), ('friend', 2059), ('fun', 2054), (\"i'll\", 1947), ('next', 1936), ('show', 1920), ('yeah', 1906), ('already', 1872), ('damn', 1849), ('great', 1816), ('another', 1786), ('ugh', 1781), ('year', 1774), ('bore', 1772), ('early', 1766), ('soon', 1716), ('yet', 1710), ('head', 1701), ('break', 1695), ('phone', 1686), ('away', 1685), ('cold', 1673), ('play', 1665), ('tweet', 1634), ('use', 1622), ('little', 1619), ('keep', 1578), ('monday', 1554), ('man', 1524), ('life', 1521), ('end', 1519), ('do', 1508), ('help', 1508), ('poor', 1494), ('guy', 1490), ('call', 1483), ('someone', 1482), ('tell', 1474), ('something', 1465), ('nice', 1460), ('wake', 1457), ('please', 1444), ('happy', 1441), ('give', 1437), ('house', 1436), ('ok', 1425), ('late', 1406), ('ready', 1406), ('first', 1404), ('study', 1385), ('talk', 1378), ('hard', 1371), ('omg', 1371), ('always', 1345), ('happen', 1344), ('guess', 1342), ('hear', 1331), ('mean', 1331), ('nothing', 1324), ('cry', 1318), ('finish', 1311), ('old', 1310), ('sunday', 1310), ('eat', 1309), ('ever', 1298), ('let', 1284), ('exam', 1283), ('game', 1273), ('fuck', 1263), ('stop', 1259), ('lot', 1241), ('you', 1235), ('weather', 1234), ('die', 1229), ('yes', 1214), ('girl', 1213), ('live', 1213), ('car', 1205), ('run', 1197), ('yesterday', 1185), ('since', 1182), ('seem', 1176), ('big', 1174), ('everyone', 1153), ('stay', 1149), ('thanks', 1143), ('bit', 1130), ('sure', 1115), ('gotta', 1115), ('baby', 1113), ('headache', 1102), ('shit', 1097), ('mom', 1095), ('stupid', 1094), ('buy', 1079), ('movie', 1067), ('didnt', 1053), ('two', 1050), ('maybe', 1049), ('best', 1046), ('read', 1041), ('pretty', 1034), (':(', 1026), ('anymore', 1022), ('follow', 1009), ('stuff', 1002), ('many', 998), ('sit', 996), ('believe', 993), ('class', 992), ('anything', 989), ('hot', 989), ('put', 983), ('may', 977), ('also', 976), ('tho', 975), ('cause', 950), ('sound', 947), ('must', 947), ('outside', 942), ('hey', 939), ('move', 938), ('without', 933), ('write', 924), ('clean', 919), ('ill', 909), ('actually', 906), ('homework', 898), ('might', 897), ('wrong', 891), ('till', 885), ('almost', 883), ('saw', 879), ('god', 878), ('sun', 878), ('kill', 877), ('ur', 876), ('wow', 871), ('party', 870), ('job', 865), ('pain', 861), ('fail', 858), ('around', 851), ('far', 849), ('drive', 849), ('month', 846), ('boo', 844), ('<3', 843), ('finally', 841), ('awake', 841), ('kid', 837), ('summer', 837), ('dog', 831), ('later', 817), ('turn', 807), ('friday', 806), ('least', 795), ('enough', 791), ('anyone', 791), (\"he's\", 790), ('forget', 784), ('test', 779), ('food', 779), ('awesome', 778), ('room', 777), ('money', 773), ('walk', 768), ('send', 761), ('alone', 760), ('close', 759), ('cool', 755), ('final', 753), ('minute', 749), ('feeling', 746), ('sore', 739), ('holiday', 739), ('probably', 737), ('update', 735), ('place', 732), ('song', 731), ('listen', 731), ('pic', 726), ('everything', 723), ('plan', 722), (\"i'd\", 721), ('throat', 720), ('coffee', 720), ('able', 717), ('internet', 717), ('crap', 717), ('wonder', 715), ('saturday', 712), ('mine', 709), ('computer', 708), ('every', 705), ('fall', 705), ('office', 704), ('idea', 702), ('busy', 702), ('woke', 696), ('eye', 694), ('check', 691), ('kinda', 691), ('family', 690), ('hell', 689), ('book', 689), ('win', 686), ('world', 684), ('hair', 684), ('sooo', 677), ('change', 677), ('whole', 676), ('10', 675), ('post', 672), ('either', 668), ('dream', 666), ('sigh', 665), ('face', 665), ('asleep', 663), ('wont', 663), ('drink', 662), ('season', 662), ('tv', 659), ('totally', 658), ('okay', 658), ('half', 653), ('yay', 652), ('enjoy', 652), ('lunch', 647), ('suppose', 644), ('hit', 640), ('follower', 637), ('hungry', 635), ('boy', 634), ('luck', 633), ('forward', 632), ('real', 631), (\"there's\", 629), ('meet', 626), ('ago', 614), ('tired', 614), ('flu', 608), ('music', 601), ('open', 600), ('rest', 598), ('unfortunately', 597), ('mother', 597), ('cat', 595), ('video', 592), ('dinner', 581), ('heart', 580), ('ticket', 578), ('problem', 577), ('stick', 574), ('else', 572), ('ya', 571), ('reply', 571), ('part', 571), ('dad', 569), ('free', 568), ('cuz', 565), ('paper', 565), ('fix', 564), ('picture', 563), ('dead', 562), ('cut', 557), ('catch', 555), ('crazy', 554), ('remember', 551), ('math', 550), ('news', 548), ('hop', 544), (\"what's\", 538), ('xx', 535), ('full', 532), ('bring', 532), ('instead', 530), ('super', 529), ('hand', 528), ('soo', 524), ('ask', 521), ('seriously', 521), ('hopefully', 519), ('pay', 518), ('birthday', 515), ('laptop', 513), ('spend', 507), ('horrible', 507), ('upset', 506), ('beautiful', 506), ('due', 506), ('hug', 502), ('iphone', 498), ('online', 497), ('jealous', 493), ('aw', 492), ('name', 491), (\"we're\", 491), ('brother', 491), ('til', 489), ('lonely', 488), ('trip', 483), ('foot', 481), ('cancel', 478), ('shop', 473), ('link', 471), ('concert', 470), ('email', 468), ('drop', 468), ('breakfast', 467), ('fan', 465), ('glad', 464), ('wear', 463), ('college', 463), ('mad', 462), ('hang', 461), ('kind', 460), ('ah', 460), ('sunny', 460), ('mind', 457), ('sell', 457), ('shower', 456), (\"they're\", 454), ('sleepy', 454), ('stomach', 453), ('ive', 452), ('revision', 451), ('fell', 450), ('rainy', 448), ('reason', 447), ('train', 444), ('beach', 444), ('text', 444), ('sadly', 442), ('lucky', 442), ('thank', 441), ('pack', 440), ('word', 439), ('english', 437), ('doesnt', 435), ('boring', 434), ('shame', 431), ('soooo', 427), ('bum', 426), ('sister', 425), ('page', 424), ('bus', 424), ('pick', 422), ('ache', 421), ('care', 420), ('second', 419), ('fast', 417), ('message', 417), ('stuck', 417), ('person', 417), ('hi', 417), ('short', 414), ('min', 413), ('moment', 412), ('lovely', 412), ('sometimes', 412), ('slow', 411), ('window', 411), ('realize', 410), ('site', 404), ('bye', 404), ('ipod', 404), ('true', 404), ('excite', 403), ('quite', 403), ('thought', 402), ('learn', 401), ('water', 401), ('star', 400), ('load', 400), ('bout', 399), ('store', 399), ('mum', 399), ('dear', 398), ('tummy', 398), ('anyway', 398), ('church', 398), ('cannot', 393), ('dude', 392), ('mood', 392), ('photo', 387), ('worry', 387), ('set', 386), ('afternoon', 385), ('scar', 384), ('figure', 384), ('funny', 383), ('depress', 382), ('tuesday', 382), ('episode', 380), ('rather', 378), ('weird', 377), ('30', 377), ('past', 376), ('wtf', 373), ('annoy', 372), ('as', 371), ('amaze', 371), ('mac', 370), ('point', 370), ('body', 369), ('gym', 369), ('ate', 369), ('yea', 369), ('save', 368), ('apparently', 368), ('project', 367), ('couple', 367), ('facebook', 365), ('meeting', 365), ('town', 364), ('ouch', 364)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(merge_lists(tweets)).most_common(500)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        target                                             tweets\n",
      "0            0  [@, httpLink, aww, that's, bummer, shoulda, ge...\n",
      "1            0  [upset, can't, update, facebook, texting, ...,...\n",
      "2            0  [@, dive, many, time, ball, manage, save, 50, ...\n",
      "3            0             [whole, body, feel, itchy, like, fire]\n",
      "4            0                  [@, behave, i'm, mad, can't, see]\n",
      "...        ...                                                ...\n",
      "159995       0                       [home, bore, thinking, life]\n",
      "159996       0  [@, wish, could, tell, stop, tweeting, cause, ...\n",
      "159997       0  [london, mo, ridiculously, hot, leave, short, ...\n",
      "159998       0  [one, day, san, jose, we're, officially, vacat...\n",
      "159999       0  [@, kind, right, though, ember, ..., sometimes...\n",
      "\n",
      "[160000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#tw = raw_tweets_df.drop(columns=[1,2,3,4,5])\n",
    "tw = raw_tweets_df[:slice_].drop(columns=[1,2,3,4,5])\n",
    "tw[\"target\"] =tw[0]\n",
    "tw = tw.drop(columns=[0])\n",
    "tw[\"tweets\"] = tweets\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/processed_tweets.csv\"):\n",
    "    os.remove(\"data/processed_tweets.csv\")\n",
    "\n",
    "tw.to_csv(\"data/processed_tweets.csv\", encoding = \"ISO-8859-1\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        target                                             tweets\n",
      "0            0  [@, httpLink, aww, that's, bummer, shoulda, ge...\n",
      "1            0  [upset, can't, update, facebook, texting, ...,...\n",
      "2            0  [@, dive, many, time, ball, manage, save, 50, ...\n",
      "3            0             [whole, body, feel, itchy, like, fire]\n",
      "4            0                  [@, behave, i'm, mad, can't, see]\n",
      "...        ...                                                ...\n",
      "159995       0                       [home, bore, thinking, life]\n",
      "159996       0  [@, wish, could, tell, stop, tweeting, cause, ...\n",
      "159997       0  [london, mo, ridiculously, hot, leave, short, ...\n",
      "159998       0  [one, day, san, jose, we're, officially, vacat...\n",
      "159999       0  [@, kind, right, though, ember, ..., sometimes...\n",
      "\n",
      "[160000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"data/processed_tweets.csv\", encoding = \"ISO-8859-1\")\n",
    "tweets = str_list_to_list(tweets)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0    1    2    3    4         5    6         7    8    9    ...  246  \\\n",
      "0       0.0  0.0  0.0  0.0  0.0  0.288675  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "1       0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "2       0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "3       0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "4       0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "...     ...  ...  ...  ...  ...       ...  ...       ...  ...  ...  ...  ...   \n",
      "159995  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "159996  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "159997  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "159998  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.316228  0.0  0.0  ...  0.0   \n",
      "159999  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
      "\n",
      "        247  248  249  250      251  252  253  254       255  \n",
      "0       0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "1       0.0  0.0  0.0  0.0  0.27735  0.0  0.0  0.0  0.000000  \n",
      "2       0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "3       0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "4       0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "...     ...  ...  ...  ...      ...  ...  ...  ...       ...  \n",
      "159995  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "159996  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "159997  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "159998  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.000000  \n",
      "159999  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.301511  \n",
      "\n",
      "[160000 rows x 256 columns]\n"
     ]
    }
   ],
   "source": [
    "#Run either hash or tfidf. You can run both but you might not have enough ram\n",
    "tweetsVector = scikit_hash_vectorize(tweets[\"tweets\"], 256)\n",
    "if os.path.exists(\"data/vec.csv\"):\n",
    "    os.remove(\"data/vec.csv\")\n",
    "tweetsVector.to_csv(\"data/vec.csv\", encoding = \"ISO-8859-1\", index=False)\n",
    "print(tweetsVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ...         @  back     can't       day      feel       get  \\\n",
      "0       0.000000  0.380508   0.0  0.000000  0.703138  0.000000  0.600675   \n",
      "1       0.416314  0.000000   0.0  0.635806  0.000000  0.000000  0.000000   \n",
      "2       0.000000  0.358120   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "3       0.000000  0.000000   0.0  0.000000  0.000000  0.726113  0.000000   \n",
      "4       0.000000  0.373908   0.0  0.722608  0.000000  0.000000  0.000000   \n",
      "...          ...       ...   ...       ...       ...       ...       ...   \n",
      "159995  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "159996  0.000000  0.349693   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "159997  0.000000  0.000000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "159998  0.000000  0.000000   0.0  0.000000  0.566589  0.000000  0.484024   \n",
      "159999  0.930032  0.367479   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "              go  good       i'm  ...  really  sad  still  think      time  \\\n",
      "0       0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "1       0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "2       0.559655   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.747353   \n",
      "3       0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "4       0.000000   0.0  0.581403  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "...          ...   ...       ...  ...     ...  ...    ...    ...       ...   \n",
      "159995  0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "159996  0.000000   0.0  0.543750  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "159997  0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "159998  0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "159999  0.000000   0.0  0.000000  ...     0.0  0.0    0.0    0.0  0.000000   \n",
      "\n",
      "           today      want  well      wish  work  \n",
      "0       0.000000  0.000000   0.0  0.000000   0.0  \n",
      "1       0.649949  0.000000   0.0  0.000000   0.0  \n",
      "2       0.000000  0.000000   0.0  0.000000   0.0  \n",
      "3       0.000000  0.000000   0.0  0.000000   0.0  \n",
      "4       0.000000  0.000000   0.0  0.000000   0.0  \n",
      "...          ...       ...   ...       ...   ...  \n",
      "159995  0.000000  0.000000   0.0  0.000000   0.0  \n",
      "159996  0.000000  0.000000   0.0  0.762922   0.0  \n",
      "159997  0.701670  0.712502   0.0  0.000000   0.0  \n",
      "159998  0.000000  0.000000   0.0  0.000000   0.0  \n",
      "159999  0.000000  0.000000   0.0  0.000000   0.0  \n",
      "\n",
      "[160000 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "#You need to experiment with values. The lower the better, but you might not have enough ram.\n",
    "#If you don't have enough RAM it will cause jupyter-notebook kernel to die\n",
    "tweetsTfidfVector = scikit_vectorize(tweets[\"tweets\"], 5000)\n",
    "if os.path.exists(\"data/vecTfidf.csv\"):\n",
    "    os.remove(\"data/vecTfidf.csv\")\n",
    "tweetsTfidfVector.to_csv(\"data/vecTfidf.csv\", encoding = \"ISO-8859-1\", index=False)\n",
    "print(tweetsTfidfVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
