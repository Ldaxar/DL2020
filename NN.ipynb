{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Adam Lewandowski, Ivan Sladkov, Patrick English\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_list_to_list(tweets):\n",
    "    t = tweets.copy()\n",
    "    import ast \n",
    "    t[\"tweets\"] = t[\"tweets\"].apply(lambda x: ast.literal_eval(x))\n",
    "    return t\n",
    "#Download this: https://drive.google.com/open?id=1-3lesjyVd1gGnjJGz_cipqO8CNeiTiPx\n",
    "#Put it into data folder\n",
    "tweets = pd.read_csv(\"data/processed_tweets.csv\", encoding = \"ISO-8859-1\")\n",
    "tweets = str_list_to_list(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         target                                             tweets\n",
      "0             0  [@switchfoot, http://twitpic.com/2y1zl, awww, ...\n",
      "1             0  [upset, can't, update, facebook, texting, ...,...\n",
      "2             0  [@kenichan, dive, many, time, ball, manage, sa...\n",
      "3             0             [whole, body, feel, itchy, like, fire]\n",
      "4             0   [@nationwideclass, behave, i'm, mad, can't, see]\n",
      "...         ...                                                ...\n",
      "1599995       4                [woke, school, best, feeling, ever]\n",
      "1599996       4  [thewdb.com, cool, hear, old, walt, interview,...\n",
      "1599997       4               [ready, mojo, makeover, ask, detail]\n",
      "1599998       4  [happy, 38th, birthday, boo, alll, time, tupac...\n",
      "1599999       4  [happy, #charitytuesday, @thenspcc, @sparkscha...\n",
      "\n",
      "[1600000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(df, dictionary_size):\n",
    "    # Pre-processing for word embeddings\n",
    "    # Count each unique word\n",
    "    raw_words = tweets['tweets']\n",
    "    word_count = {}\n",
    "    for tweet in raw_words:\n",
    "        for word in tweet:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    # Sort each unique word using the value\n",
    "    sorted_word_count = sorted(word_count.items(), key=lambda key: key[1], reverse=True)\n",
    "    \n",
    "    # create dictionaries to convert tokens to integers and integers to tokens (needed for the embedding layer)\n",
    "    # <pad> will be used for padding the data because the NN inputs have to be the same size and tweets are of varying length\n",
    "    # <unk> will be used to replace tokens that were too uncommon to add to the dictionary\n",
    "    word_dictionary={'<pad>': 0, '<unk>': 1}\n",
    "    for i in range(dictionary_size):\n",
    "        word_dictionary[sorted_word_count[i][0]]=i+2\n",
    "    reverse_dictionary = dict([(value, key) for (key, value) in word_dictionary.items()])\n",
    "\n",
    "    # Convert each token into its index in the dictionary and\n",
    "    # create a new dataframe with the list of indexes and corresponding targets\n",
    "    data=[]\n",
    "    \n",
    "    for tweet in tweets['tweets']:\n",
    "        data.append(parse_tweet(tweet, word_dictionary))\n",
    "\n",
    "    input_data=pd.DataFrame({\n",
    "        'data': data,\n",
    "        'targets': tweets['target']\n",
    "    })\n",
    "    \n",
    "    return input_data, word_dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_padded_data(data, seed):\n",
    "    # Get the length of the longest tweet\n",
    "    max_tweet_length=len(max(data['data'], key=len))\n",
    "    # Split the data 80% train data (later a subset will be selected as validation data) - 20% test data\n",
    "    shuffled_dataframe=shuffle(data, random_state=seed)\n",
    "    train_dataframe=shuffled_dataframe.sample(frac=0.8, random_state=seed)\n",
    "    test_dataframe=shuffled_dataframe.drop(train_dataframe.index)\n",
    "    \n",
    "    # Pad the tweets with <pad> up to the max tweet length so all tweets have the same length\n",
    "    train_data=keras.preprocessing.sequence.pad_sequences(list(train_dataframe['data']),\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=max_tweet_length)\n",
    "    train_targets=train_dataframe['targets'].replace(4,1)\n",
    "    \n",
    "    test_data=keras.preprocessing.sequence.pad_sequences(list(test_dataframe['data']),\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=max_tweet_length)\n",
    "    test_targets=test_dataframe['targets'].replace(4,1)\n",
    "    \n",
    "    return train_data, train_targets, test_data, test_targets, max_tweet_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_data, train_targets, word_dictionary, max_tweet_length, seed):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(len(word_dictionary), output_dim=150, input_length=max_tweet_length, trainable=True))\n",
    "    model.add(keras.layers.LSTM(80))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables used for pre-processing the data\n",
    "dictionary_size=25000\n",
    "seed=2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data.\n",
    "# Create the dictionaries, split and pad the data\n",
    "input_data, word_dictionary, reverse_dictionary = prepare_input_data(tweets, dictionary_size)\n",
    "train_data, train_targets, test_data, test_targets, max_tweet_length = split_padded_data(input_data, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=create_model(train_data, train_targets, word_dictionary, max_tweet_length, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1152000 samples, validate on 128000 samples\n",
      "Epoch 1/3\n",
      "1152000/1152000 [==============================] - 1755s 2ms/sample - loss: 0.4738 - accuracy: 0.7680 - val_loss: 0.4480 - val_accuracy: 0.7892\n",
      "Epoch 2/3\n",
      "1152000/1152000 [==============================] - 1738s 2ms/sample - loss: 0.4250 - accuracy: 0.8021 - val_loss: 0.4378 - val_accuracy: 0.7944\n",
      "Epoch 3/3\n",
      "1152000/1152000 [==============================] - 1683s 1ms/sample - loss: 0.4019 - accuracy: 0.8154 - val_loss: 0.4380 - val_accuracy: 0.7946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x158927ae4c8>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_targets, epochs=3,batch_size=50, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000/320000 [==============================] - 89s 279us/sample - loss: 0.4385 - accuracy: 0.7948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43850039470940827, 0.7947969]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, test_targets, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
