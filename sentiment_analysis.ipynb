{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Adam Lewandowski, Ivan Sladkov, Patrick English\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "#from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/training.1600000.processed.noemoticon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-521143b8c869>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_tweets_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/training.1600000.processed.noemoticon.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw_tweets_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i_sla\\anaconda3\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i_sla\\anaconda3\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i_sla\\anaconda3\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i_sla\\anaconda3\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i_sla\\anaconda3\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/training.1600000.processed.noemoticon.csv'"
     ]
    }
   ],
   "source": [
    "raw_tweets_df = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", header=None)\n",
    "raw_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          1467810369\n",
      "1          1467810672\n",
      "2          1467810917\n",
      "3          1467811184\n",
      "4          1467811193\n",
      "              ...    \n",
      "1599995    2193601966\n",
      "1599996    2193601969\n",
      "1599997    2193601991\n",
      "1599998    2193602064\n",
      "1599999    2193602129\n",
      "Name: 1, Length: 1600000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(raw_tweets_df[1])\n",
    "#Baseline processing\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "#This can take a while\n",
    "#tokenized_tweets = raw_tweets_df[5].copy().apply(lambda x : x.lower())\n",
    "\n",
    "# If you don't want to wait\n",
    "tokenized_tweets = raw_tweets_df[5][:3200].copy().apply(lambda x : x.lower().encode('ascii', 'ignore'))\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x : tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gdfgfd', 'rer']\n"
     ]
    }
   ],
   "source": [
    "#Should things like ... stay? \n",
    "def remove_punctuation(tweets):\n",
    "    t = tweets.copy()\n",
    "    def remove_punct(tokens):\n",
    "        if '.' in tokens:\n",
    "            tokens.remove('.')\n",
    "        if '?' in tokens:\n",
    "            tokens.remove('?')\n",
    "        if '!' in tokens:\n",
    "            tokens.remove('!')\n",
    "        if ',' in tokens:\n",
    "            tokens.remove(',')\n",
    "        if '\\'' in tokens:\n",
    "            tokens.remove('\\'')\n",
    "        if '\"' in tokens:\n",
    "            tokens.remove('\"')\n",
    "        if '#' in tokens:\n",
    "            tokens.remove('#')\n",
    "        if '' in tokens:\n",
    "            tokens.remove('#')\n",
    "            \n",
    "        return tokens\n",
    "    return t.apply(lambda tweet : remove_punct(tweet))\n",
    "\n",
    "def remove_stop_words(tweets):\n",
    "    t = tweets.copy()\n",
    "    from nltk.corpus import stopwords\n",
    "    sw = set(stopwords.words('english')) \n",
    "    return t.apply(lambda tweet:[w for w in tweet if not w in sw])\n",
    "\n",
    "#TODO add spell correction\n",
    "def correct_spelling(tweets):\n",
    "    return tweets\n",
    "\n",
    "def pos_tag(tweets):\n",
    "    t = tweets.copy()\n",
    "    return t.apply(lambda tweet: nltk.pos_tag(tweet))\n",
    "\n",
    "#TBD\n",
    "def emoticon_transcoder(tweets):\n",
    "    return tweets\n",
    "\n",
    "def remove_pos_tags(tweets):\n",
    "    t = tweets.copy()\n",
    "    return t.apply(lambda tweet: [w[0] for w in tweet])\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "def scikit_vectorize(tweets, min_freq):\n",
    "    t = tweets.copy()\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, tokenizer=identity_tokenizer, min_df=min_freq, dtype=np.float32)\n",
    "    vectors = vectorizer.fit_transform(t)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    return pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "def scikit_hash_vectorize(tweets):\n",
    "    t = tweets.copy()\n",
    "    vectorizer = HashingVectorizer(lowercase=False, n_features=2**8, tokenizer=identity_tokenizer, dtype=np.float32)\n",
    "    vectors = vectorizer.fit_transform(t)\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    return pd.DataFrame(denselist)\n",
    "\n",
    "def str_list_to_list(tweets):\n",
    "    t = tweets.copy()\n",
    "    import ast \n",
    "    t[\"tweets\"] = t[\"tweets\"].apply(lambda x: ast.literal_eval(x))\n",
    "    return t\n",
    "\n",
    "def lemmatize(tweets):\n",
    "    t = tweets.copy()\n",
    "    \n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        from nltk.corpus import wordnet\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    lem = WordNetLemmatizer()\n",
    "    def lemmatize_word(w):\n",
    "        transcoded_tag = get_wordnet_pos(w[1])\n",
    "        wl = lem.lemmatize(w[0], transcoded_tag)\n",
    "        return (wl,w[1])\n",
    "    return t.apply(lambda tweet:[lemmatize_word(w) for w in tweet])\n",
    "\n",
    "def remove_punct(tokens):\n",
    "        if '.' in tokens:\n",
    "            tokens.remove('.')\n",
    "        if '?' in tokens:\n",
    "            tokens.remove('?')\n",
    "        if '!' in tokens:\n",
    "            tokens.remove('!')\n",
    "        if ',' in tokens:\n",
    "            tokens.remove(',')\n",
    "        return tokens\n",
    "    \n",
    "def remove_one_char_trash(tweets):\n",
    "    return tweets.apply(lambda tweet: [s for s in tweet if len(s) >= 2])\n",
    "\n",
    "arr = [\"gdfgfd\", \"]\",\"rer\"]\n",
    "arr = [s for s in arr if len(s) >= 2]\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "#comment lines to skip some steps\n",
    "#You need to run things in this order\n",
    "tweets = remove_punctuation(tokenized_tweets)\n",
    "tweets = remove_one_char_trash(tweets)\n",
    "tweets = remove_stop_words(tweets)\n",
    "tweets = pos_tag(tweets)\n",
    "tweets = lemmatize(tweets)\n",
    "tweets = remove_pos_tags(tweets)\n",
    "#tweets = merge_tokens(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         target                                             tweets\n",
      "0             0  [@switchfoot, http://twitpic.com/2y1zl, awww, ...\n",
      "1             0  [upset, can't, update, facebook, texting, ...,...\n",
      "2             0  [@kenichan, dive, many, time, ball, manage, sa...\n",
      "3             0             [whole, body, feel, itchy, like, fire]\n",
      "4             0   [@nationwideclass, behave, i'm, mad, can't, see]\n",
      "...         ...                                                ...\n",
      "1599995       4                [woke, school, best, feeling, ever]\n",
      "1599996       4  [thewdb.com, cool, hear, old, walt, interview,...\n",
      "1599997       4               [ready, mojo, makeover, ask, detail]\n",
      "1599998       4  [happy, 38th, birthday, boo, alll, time, tupac...\n",
      "1599999       4  [happy, #charitytuesday, @thenspcc, @sparkscha...\n",
      "\n",
      "[1600000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tw = raw_tweets_df.drop(columns=[1,2,3,4,5])\n",
    "tw[\"target\"] =tw[0]\n",
    "tw = tw.drop(columns=[0])\n",
    "tw[\"tweets\"] = tweets\n",
    "print(tw)\n",
    "#print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "if os.path.exists(\"processed_tweets.csv\"):\n",
    "    os.remove(\"processed_tweets.csv\")\n",
    "\n",
    "tw.to_csv(\"processed_tweets.csv\", encoding = \"ISO-8859-1\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"processed_tweets.csv\", encoding = \"ISO-8859-1\")\n",
    "tweets = str_list_to_list(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         target                                             tweets\n",
      "0             0  [@switchfoot, http://twitpic.com/2y1zl, awww, ...\n",
      "1             0  [upset, can't, update, facebook, texting, ...,...\n",
      "2             0  [@kenichan, dive, many, time, ball, manage, sa...\n",
      "3             0             [whole, body, feel, itchy, like, fire]\n",
      "4             0   [@nationwideclass, behave, i'm, mad, can't, see]\n",
      "...         ...                                                ...\n",
      "1599995       4                [woke, school, best, feeling, ever]\n",
      "1599996       4  [thewdb.com, cool, hear, old, walt, interview,...\n",
      "1599997       4               [ready, mojo, makeover, ask, detail]\n",
      "1599998       4  [happy, 38th, birthday, boo, alll, time, tupac...\n",
      "1599999       4  [happy, #charitytuesday, @thenspcc, @sparkscha...\n",
      "\n",
      "[1600000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweets)\n",
    "#tweetsVector = scikit_vectorize(tweets, 2)\n",
    "\n",
    "#print(tweets.array)\n",
    "#tweetsVector\n",
    "#print(tweetsVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tweetsVector = scikit_hash_vectorize(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"vec.csv\"):\n",
    "    os.remove(\"vec.csv\")\n",
    "\n",
    "tweetsVector.to_csv(\"vec.csv\", encoding = \"ISO-8859-1\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetsVector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b7ebed553ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetsVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(tweetsVector.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetsVector' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(tweetsVector.iloc[0]))\n",
    "#print(tweetsVector.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0    1    2    3    4         5         6    7    8    9    ...  246  \\\n",
      "0        0.0  0.0  0.0  0.0  0.0  0.288675  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "1        0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "2        0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "3        0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "4        0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "...      ...  ...  ...  ...  ...       ...       ...  ...  ...  ...  ...  ...   \n",
      "1599995  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "1599996  0.0  0.0  0.0  0.0  0.0  0.000000 -0.377964  0.0  0.0  0.0  ...  0.0   \n",
      "1599997  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "1599998  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "1599999  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "\n",
      "         247       248       249  250      251  252  253  254  255  \n",
      "0        0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "1        0.0  0.000000  0.000000  0.0  0.27735  0.0  0.0  0.0  0.0  \n",
      "2        0.0  0.000000  0.333333  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "3        0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "4        0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "...      ...       ...       ...  ...      ...  ...  ...  ...  ...  \n",
      "1599995  0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "1599996  0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "1599997  0.0  0.377964  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "1599998  0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "1599999  0.0  0.000000  0.000000  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1600000 rows x 256 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweetsVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
